
# NLP

Continued work loading an edition of the Equity and then doing NER on it based on https://rpubs.com/lmullen/nlp-chapter

## Idea: Use the entities from NER to make another finding aid.

### Need a database to record information that can be migrated to the web later.

Selected MySql given I can run it locally on my machine and then move it to my website (and it's free)

Installed MySql locally.

## Connect R to MySQL to save data

Used these instructions

https://www.r-bloggers.com/accessing-mysql-through-r/

I'm having a database result set issue with the syntax I'm using.  I will have to pick this up later

     Error in .local(conn, statement, ...) : 
       could not run statement: Duplicate entry '83471_1883-06-14' for key 'sourcedocumentname_UNIQUE'
     In addition: Warning messages:
     1: In if (dbRows[[1]] == 0) { :
       the condition has length > 1 and only the first element will be used
     2: closing unused connection 4 (equityurls_test2.txt) 
     3: closing unused connection 3 (equityurls_test2.txt) 
     4: In if (dbRows[[1]] == 0) { :
       the condition has length > 1 and only the first element will be used

17 Sept 2017 - I was able to solve this, I am rusty on SQL.  I am now challenged on how to use db log in credentials in a file and not have them in my code.  I see how to do this, but can't make it work yet.

## Deal with entities:

sort(unique(entities(equityEdition_doc, kind = "location")))

Interesting Error.  I ran through the editions of 1970 and I got an error:

     Error in .jnew("opennlp.tools.namefind.TokenNameFinderModel", .jcast(.jnew("java.io.FileInputStream",  : 
     java.lang.OutOfMemoryError: GC overhead limit exceeded

This is supposed to help manage that, but I got the error again

options(java.parameters = "- Xmx1024m")

MySQL error inserting some entities due to special characters due to bad OCR results

      Error in .local(conn, statement, ...) : 
         could not run statement: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ''V.\'' at line 1 

I am coping with this with some gsub to replace the characters that are causing problems for SQL

# handling out of memory errors until I fix them

Due to out of memory errors I am recording when each issue is processed in the database. Then if I need to re-start the process the program will skip the items already processed.

I removed some of the dbClearResult(rs) statements with the theory that this was causing the underlying Java to do garbage collection and as the error says GC overhead limit exceeded.

     Error in .jnew("opennlp.tools.namefind.TokenNameFinderModel", .jcast(.jnew("java.io.FileInputStream",  : 
     java.lang.OutOfMemoryError: GC overhead limit exceeded

This seemed to work better.

Later I got an error message:

    java.lang.OutOfMemoryError: Java heap space
    
So I upped my memory usage here:    

     options(java.parameters = "-Xmx4096m")

Per this article:  http://javarevisited.blogspot.ca/2011/09/javalangoutofmemoryerror-permgen-space.html  I tried this

     options(java.parameters = "-Xms4096m -Xmx4096m")

I still got java.lang.OutOfMemoryError: GC overhead limit exceeded

I commented out all of the output to html files which was part of the original program.  This seems to work better.  These files were large (maybe too large)  Also, with the re-starting of the program, they were incomplete becuase they onl had results from the most recent run of the program.  Finally, I will re-structure the program to store the info in a database and then run an second program to make the out put.  (likely in markdown and then I will convertthat with pandoc)

#outputFilePeopleHtml <- "equityeditions_people.html"
#outputFilePeopleHtmlCon<-file(outputFilePeopleHtml, open = "w")

## Running 2 different programs in R

I have 2 long running programs - I tried to start the second program in R studio and not much happened.  It turns out the program gets queued and runs after the other finishes.  To get around this I opened a second R studio and ran the second program.

### Forcing garbage collection

Added 

https://stackoverflow.com/questions/1467201/forcing-garbage-collection-to-run-in-r-with-the-gc-command

gc()
